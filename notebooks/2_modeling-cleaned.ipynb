{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57cab362-f9ea-4150-afee-54b424f8757f",
   "metadata": {},
   "source": [
    "# Modeling Notebook\n",
    "\n",
    "- **Creation Date**: June 13, 2025  \n",
    "- **Author**: Corentin Vasseur â€” [vasseur.corentin@gmail.com](mailto:vasseur.corentin@gmail.com)\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we present a baseline model along with several variations that were tested. We include the performance metrics used to evaluate them, as well as the details of hyperparameter tuning.\n",
    "\n",
    "Finally, we propose several improvements to enhance model performance based on our findings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303482e3-3746-4966-a57c-945a1d0c2d52",
   "metadata": {},
   "source": [
    "## 1. Imports section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "255e7b76-5769-4be6-b16a-843703acf619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from skrub import GapEncoder\n",
    "from skrub import Cleaner, TableReport\n",
    "from skrub import StringEncoder, MinHashEncoder, TableVectorizer, TextEncoder\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from qto_categorizer_ml.io.datasets import CSVReader\n",
    "from qto_categorizer_ml.io.services import LoggerService\n",
    "from qto_categorizer_ml.core.models import BaselineModel, SKLearnPipelineModel\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "logger = LoggerService().logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0899deb-ab0f-4063-9094-df9ca50dfff2",
   "metadata": {},
   "source": [
    "## 2. Load datasets \n",
    "\n",
    "For this section we use [CSVReader](https://github.com/data-corentinv/qto-categorizer-ml) object (from `qto-categorized-ml` pacakage). The idea behind is to separate and manage different method to read data from local, s3 bucket, deltalake, etc. on different environement (local, dev, preprod, production)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cda9799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'TRANSACTION_ID': str,\n",
    "    'AMOUNT': float,\n",
    "    'TYPE_OF_PAYMENT': str,\n",
    "    'MERCHANT_NAME': str,\n",
    "    'DESCRIPTION': str,\n",
    "    'SIDE':  int,\n",
    "    'CATEGORY': str,\n",
    "}\n",
    "\n",
    "parse_dates = ['DATE_EMITTED']\n",
    "\n",
    "path = \"../data/data-products.csv\"\n",
    "df = CSVReader(path=path, dtypes=dtypes, parse_dates=parse_dates).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5918c0ac-9e68-4f1b-9474-8a965803991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in MERCHANT_NAME, DESCRIPTION, TYPE_OF_PAYMENT\n",
    "\n",
    "df['MERCHANT_NAME'] = df.MERCHANT_NAME.fillna(\"\")\n",
    "df['DESCRIPTION'] = df.DESCRIPTION.fillna(\"\")\n",
    "df['TYPE_OF_PAYMENT'] = df.TYPE_OF_PAYMENT.fillna(\"\")\n",
    "\n",
    "# Features selection\n",
    "features = ['AMOUNT', 'TYPE_OF_PAYMENT', 'MERCHANT_NAME', 'DESCRIPTION']\n",
    "target = 'CATEGORY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0619e60d-9b50-4b0a-a8e9-706aa2d91948",
   "metadata": {},
   "source": [
    "## 3. Create train et test datasets\n",
    "\n",
    "In this section we encode target for training and split X and y datasets to make attention to respect distribution of target (`CATEGORY` feature) in both dataset.\n",
    "\n",
    "ADD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6b9dddb6-fd54-4815-ad6b-53767792255d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-13 17:44:32.719\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mDim of train set (268729, 4), dim of test set (67183, 4)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "X = df[features+[target]].drop_duplicates()\n",
    "y = X.pop(target)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "logger.info(f\"Dim of train set {X_train.shape}, dim of test set {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622dd18-bcd4-407a-a263-95fad18ab070",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "As shown in the EDA notebook, a simple (but solid!) `baseline` is to assign each merchant their most frequent category. \n",
    "\n",
    "This rule-based approach serves as a reference point to evaluate whether our model can outperform a naive strategy that already achieves surprisingly good accuracy.\n",
    "\n",
    "In the `qto-categorizer-ml` python module, we propose an implementation in `qto_categorizer_ml.core.model.BaselineModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d47b276a-d3d5-4045-a3a7-52fade66ea51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-13 17:44:33.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mBaseline prediction example (test set): ['Operational Expenses: Office Supplies'\n",
      " 'Operational Expenses: Production Costs'\n",
      " 'Operational Expenses: Production Costs'\n",
      " 'Operational Expenses: Office Supplies'\n",
      " 'Operational Expenses: Office Supplies']...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "baseline = BaselineModel()\n",
    "baseline.fit(inputs=X_train, targets=y_train)\n",
    "\n",
    "y_baseline_pred = baseline.predict(inputs=X_test)\n",
    "logger.info(f\"Baseline prediction example (test set): {y_baseline_pred[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93728a89-c286-4388-abc7-5c646f11dc22",
   "metadata": {},
   "source": [
    "## Create pipeline\n",
    "\n",
    "This pipeline is designed to classify transactions using a mix of `numerical`(e.g. `AMOUNT`), `categorical`(e.g. `TYPE_OF_PAYMENT`), and textual features (e.g. `DESCRIPTION`). \n",
    "\n",
    "It includes preprocessing steps tailored to each type of data, followed by a `RandomForestclassifier`.\n",
    "\n",
    "### Here the details of the pipeine:\n",
    "\n",
    "**Preprocessing (via ColumnTransformer):**\n",
    "- Numerical Feature:\n",
    "    *  `AMOUNT`: Scaled using StandardScaler to normalize values (mean 0, std 1).\n",
    "Categorical Feature:\n",
    "    * `TYPE_OF_PAYMENT`: Encoded using OneHotEncoder, with unknown categories ignored at inference time.\n",
    "- Text Features:\n",
    "    * `DESCRIPTION`:\n",
    "        * Transformed using `TF-IDF` (max 1000 features).\n",
    "        * Reduced to 50 dimensions using `TruncatedSVD` (a form of PCA for sparse matrices).\n",
    "    * `MERCHANT_NAME`:\n",
    "        * Similar processing with `TF-IDF` (max 500 features).\n",
    "        * Dimensionality reduced to 30 components.\n",
    "\n",
    "**Modeling:**\n",
    "\n",
    "- Classifier: A `RandomForestClassifier` with:\n",
    "  * 200 trees (`n_estimators`=200)\n",
    "  * `Maximum depth` of 30 per tree\n",
    "  * Parallel processing (`n_jobs`=-1)\n",
    "  * Fixed randomness (`random_state`=42) for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a78c3a61-efd7-40de-8f0a-17ddb353170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = SKLearnPipelineModel()\n",
    "pipeline.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac95bf8-1afd-428a-943b-71c536b51964",
   "metadata": {},
   "source": [
    "## Find best hyperparameter\n",
    "\n",
    "Choosing the right hyperparameters can significantly improve model performance. sklearn offers several methods for automated hyperparameter tuning: [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), [RandominzedCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html), [HalvingGridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html), [HalvingRandomSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html).\n",
    "\n",
    "Best practices I used are: \n",
    "- Always combine these methods with cross-validation (cv parameter)\n",
    "- Use n_jobs=-1 to parallelize the search\n",
    "- Choose scoring metric based on the problem we want to solve (e.g. accuracy, f1_macro, roc_auc)\n",
    "\n",
    "For the categorizer, I selected RandomizedSearchCV because it efficiently explore a wide hyperparameter space with fewer computations, making it ideal for the time-constrained searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3f3b52db-65f4-4fc3-b544-67c3bf9790f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0: # replace by 1 for running the hptuning job\n",
    "    param_grid ={\n",
    "    'classifier__n_estimators': [50,100,200],\n",
    "    'classifier__max_depth': [10, 20, 30, None]\n",
    "}\n",
    "\n",
    "    grid_search = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        param_grid, \n",
    "        n_iter=10,\n",
    "        cv=3,\n",
    "        scoring=\"accuracy\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    logger.info(f\"Best params {grid_search.best_params_}, best score {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9982ba01-d250-4916-98c5-73febebb2cbe",
   "metadata": {},
   "source": [
    "## Train the pipeline\n",
    "Based on hyperparameter tuning search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553173b1-5da4-4c9f-9614-3056cb6a215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "logger.info(f\"Baseline prediction example (test set): {y_pred[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd275add-496a-44e2-91f4-dd52bf26cc03",
   "metadata": {},
   "source": [
    "## Estimate performance\n",
    "Here a description of an data science problem:\n",
    "In this section we encode target for training and split X and y datasets to make attention to respect distribution of target (CATEGORY feature) in both dataset.\n",
    "\n",
    "Explain in few sentences the fact we want to create a model for a classifcaition multicalsse\n",
    "IMPROVE\n",
    "\n",
    "We are facing to a multiclasss classification problem. We propose to used accuracy score for a global (and mean) estimation.\n",
    "In order to get details of performances on each classes, we can used classical binary metrics such as precision, recall and f1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c93b4dc-e5d4-4dbb-820e-ce692e74ffd0",
   "metadata": {},
   "source": [
    "### Accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e9d428-deff-4cdd-9f90-330cbb659928",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm, cm_baseline = \\\n",
    "    confusion_matrix(pipeline.pipeline._encode_target(y_test), pipeline.pipeline._encode_target(y_pred)), \\\n",
    "    confusion_matrix(baseline._encode_target(y_test), baseline._encode_target(y_baseline_pred))\n",
    "acc, acc_baseline = \\\n",
    "    accuracy_score(pipeline.pipeline._encode_target(y_test), pipeline.pipeline._encode_target(y_pred)), \\\n",
    "    accuracy_score(baseline._encode_target(y_test), baseline._encode_target(y_baseline_pred))\n",
    "acc, acc_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4388028b-aa14-46fe-9395-ec9469eee0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "classification_report?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72880441-2f84-4e2f-8410-ac6adb6aa973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c81e39-351d-48e7-a375-43e938544ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qto-ml",
   "language": "python",
   "name": "qto-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
